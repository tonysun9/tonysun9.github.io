---
---

@string{aps = {American Physical Society,}}



@article{sun-etal-2019-mitigating,
    title = "Mitigating Gender Bias in Natural Language Processing: Literature Review",
    author = "Sun*, Tony  and
      Gaut*, Andrew  and
      Tang, Shirlyn  and
      Huang, Yuxin  and
      ElSherief, Mai  and
      Zhao, Jieyu  and
      Mirza, Diba  and
      Belding, Elizabeth  and
      Chang, Kai-Wei  and
      Wang, William Yang",
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association of Computational Linguistics",
    journal = "ACL",
    url = "https://aclanthology.org/P19-1159",
    doi = "10.18653/v1/P19-1159",
    pages = "1630--1640",
    preview = {mitigating_preview.png},
    pdf={https://arxiv.org/pdf/1906.08976.pdf},
    abstract = "As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.",
}

@article{dhamala2021bold,
  title={BOLD: Dataset and Metrics for Measuring Biases in Open-ended Language Generation},
  author={Dhamala*, Jwala and Sun*, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  journal={ACM FAccT},
  pages={862--872},
  year={2021},
  preview={bold_preview.png},
  pdf={https://arxiv.org/pdf/2101.11718.pdf},
  abstract="Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three â€¦"
}

@article{sun2020they,
  title={They, Them, Theirs: Rewriting with Gender-neutral English},
  author={Sun, Tony and Webster, Kellie and Shah, Apu and Wang, William Yang and Johnson, Melvin},
  journal={WeCNLP, Best Paper Nomination},
  year={2020},
  preview={they_preview.png},
  pdf={https://arxiv.org/pdf/2102.06788.pdf},
  abstract="Responsible development of technology involves applications being inclusive of the diverse set of users they hope to support. An important part of this is understanding the many ways to refer to a person and being able to fluently change between the different forms as needed. We perform a case study on the singular they, a common way to promote gender inclusion in English. We define a re-writing task, create an evaluation benchmark, and show how a model can be trained to produce gender-neutral English with <1\% word error rate with no human-labeled data. We discuss the practical applications and ethical considerations of the task, providing direction for future work into inclusive natural language systems."
}